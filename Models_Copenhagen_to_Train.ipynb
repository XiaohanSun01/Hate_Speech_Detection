{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72aa9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, auc, precision_score, recall_score, f1_score, roc_auc_score, classification_report,balanced_accuracy_score, precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "import torch\n",
    "from torch import optim\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f68e63",
   "metadata": {},
   "source": [
    "# Function of Metrics Confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f11d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_confusion(y, y_pred):\n",
    "    \n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred)\n",
    "    \n",
    "\n",
    "    print('Accuracy: ', acc)\n",
    "    print('Recall: ', rec)\n",
    "    print('Precision: ', prec)\n",
    "    print('F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ebdd26",
   "metadata": {},
   "source": [
    "# Read the Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca5e0f",
   "metadata": {},
   "source": [
    "### HASOC Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24d7529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lulz new meme day opkkk optrump opdomesticterr...</td>\n",
       "      <td>#opkkk #optrump #opdomesticterrorism #fucktrum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gandinaaliabus arnob r great job showcas genui...</td>\n",
       "      <td>#gandinaaliabuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>enough harass still like unwelcom rel leav eve...</td>\n",
       "      <td>#johnmccainday #trumpisatraitor #collusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3rd class organ world biggest cup sad see help...</td>\n",
       "      <td>#icc #shameonicc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>hey love india cc dhonikeepstheglov balidaanba...</td>\n",
       "      <td>#dhonikeepstheglove #balidaanbadge #dhonikesaa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        clean_tweet  \\\n",
       "0      0  lulz new meme day opkkk optrump opdomesticterr...   \n",
       "1      1  gandinaaliabus arnob r great job showcas genui...   \n",
       "2      0  enough harass still like unwelcom rel leav eve...   \n",
       "3      0  3rd class organ world biggest cup sad see help...   \n",
       "4      0  hey love india cc dhonikeepstheglov balidaanba...   \n",
       "\n",
       "                                          Hash Words  \n",
       "0  #opkkk #optrump #opdomesticterrorism #fucktrum...  \n",
       "1                                   #gandinaaliabuse  \n",
       "2         #johnmccainday #trumpisatraitor #collusion  \n",
       "3                                   #icc #shameonicc  \n",
       "4  #dhonikeepstheglove #balidaanbadge #dhonikesaa...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasoc = pd.read_csv('data/Cleaning/HASOC_Dataset.csv',on_bad_lines='skip')\n",
    "hasoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61894ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lulz new meme day opkkk optrump opdomesticterr...</td>\n",
       "      <td>#opkkk #optrump #opdomesticterrorism #fucktrum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gandinaaliabus arnob r great job showcas genui...</td>\n",
       "      <td>#gandinaaliabuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>enough harass still like unwelcom rel leav eve...</td>\n",
       "      <td>#johnmccainday #trumpisatraitor #collusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3rd class organ world biggest cup sad see help...</td>\n",
       "      <td>#icc #shameonicc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>hey love india cc dhonikeepstheglov balidaanba...</td>\n",
       "      <td>#dhonikeepstheglove #balidaanbadge #dhonikesaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>0</td>\n",
       "      <td>leader free world find need even care mayor ci...</td>\n",
       "      <td>#pathetic #sociopathic #douchebag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>0</td>\n",
       "      <td>befit repli section medium look controversi dh...</td>\n",
       "      <td>#dhonikeepstheglove #dhonigloves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>0</td>\n",
       "      <td>heard pa clear trumpisatraitor trumplieseveryt...</td>\n",
       "      <td>#trumpisatraitor #trumplieseverytimehespeaks #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539</th>\n",
       "      <td>0</td>\n",
       "      <td>celebr putinspuppet desperatedonald trumpisatr...</td>\n",
       "      <td>#putinspuppet #desperatedonald #trumpisatraito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>0</td>\n",
       "      <td>final year obama presid gop frontrunn receiv g...</td>\n",
       "      <td>#labamba #trumpisatraitor #trumpisadisgrace #s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4541 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                        clean_tweet  \\\n",
       "0         0  lulz new meme day opkkk optrump opdomesticterr...   \n",
       "1         1  gandinaaliabus arnob r great job showcas genui...   \n",
       "2         0  enough harass still like unwelcom rel leav eve...   \n",
       "3         0  3rd class organ world biggest cup sad see help...   \n",
       "4         0  hey love india cc dhonikeepstheglov balidaanba...   \n",
       "...     ...                                                ...   \n",
       "4536      0  leader free world find need even care mayor ci...   \n",
       "4537      0  befit repli section medium look controversi dh...   \n",
       "4538      0  heard pa clear trumpisatraitor trumplieseveryt...   \n",
       "4539      0  celebr putinspuppet desperatedonald trumpisatr...   \n",
       "4540      0  final year obama presid gop frontrunn receiv g...   \n",
       "\n",
       "                                             Hash Words  \n",
       "0     #opkkk #optrump #opdomesticterrorism #fucktrum...  \n",
       "1                                      #gandinaaliabuse  \n",
       "2            #johnmccainday #trumpisatraitor #collusion  \n",
       "3                                      #icc #shameonicc  \n",
       "4     #dhonikeepstheglove #balidaanbadge #dhonikesaa...  \n",
       "...                                                 ...  \n",
       "4536                  #pathetic #sociopathic #douchebag  \n",
       "4537                   #dhonikeepstheglove #dhonigloves  \n",
       "4538  #trumpisatraitor #trumplieseverytimehespeaks #...  \n",
       "4539  #putinspuppet #desperatedonald #trumpisatraito...  \n",
       "4540  #labamba #trumpisatraitor #trumpisadisgrace #s...  \n",
       "\n",
       "[4541 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ae41fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3419\n",
       "1    1122\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasoc.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3596898",
   "metadata": {},
   "source": [
    "### Aristotle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd69393c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>take shot predict bengal first two draft day i...</td>\n",
       "      <td>#bengals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>next week localnewspap shop wednesday afternoo...</td>\n",
       "      <td>#localnewspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>final get player dnd soon align go think chaot...</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>commit oral write idea goal like honor commitm...</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>syrian alli iran blast you missil strike dange...</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        clean_tweet       Hash Words\n",
       "0      0  take shot predict bengal first two draft day i...         #bengals\n",
       "1      0  next week localnewspap shop wednesday afternoo...  #localnewspaper\n",
       "2      0  final get player dnd soon align go think chaot...      No hashtags\n",
       "3      0  commit oral write idea goal like honor commitm...      No hashtags\n",
       "4      1  syrian alli iran blast you missil strike dange...      No hashtags"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aris = pd.read_csv('data/Cleaning/Aristotle_Dataset.csv',on_bad_lines='skip')\n",
    "aris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d80c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5352\n",
       "1    1538\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aris.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29ff9f",
   "metadata": {},
   "source": [
    "### Hugging Face Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f0790f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'clean_tweet', 'Hash Words'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf = pd.read_csv('data/Cleaning/HuggingFace_Dataset.csv',on_bad_lines='skip')\n",
    "hf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98c8993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>spread positiv give someon high five today pos...</td>\n",
       "      <td>#positiveenergy #highfive #tuesdaymotivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>difficult ask mani languag measur happi unifor...</td>\n",
       "      <td>#languages- #happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>pray orlando orlando orlandostrong bestrong pr...</td>\n",
       "      <td>#orlando #orlandostrong #bestrong #pray #belie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bought thing kyli cosmet today ddd lovethemal</td>\n",
       "      <td>#lovethemall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>gorgeou modeltoi look wild sexi follow victori...</td>\n",
       "      <td>#model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        clean_tweet  \\\n",
       "0      0  spread positiv give someon high five today pos...   \n",
       "1      0  difficult ask mani languag measur happi unifor...   \n",
       "2      0  pray orlando orlando orlandostrong bestrong pr...   \n",
       "3      0      bought thing kyli cosmet today ddd lovethemal   \n",
       "4      0  gorgeou modeltoi look wild sexi follow victori...   \n",
       "\n",
       "                                          Hash Words  \n",
       "0       #positiveenergy #highfive #tuesdaymotivation  \n",
       "1                             #languages- #happiness  \n",
       "2  #orlando #orlandostrong #bestrong #pray #belie...  \n",
       "3                                       #lovethemall  \n",
       "4                                             #model  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50c12f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25255\n",
       "1     1676\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e75e0",
   "metadata": {},
   "source": [
    "### Copenhagen Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d628a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ok enough instant restaur alreadi</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>femal cowork asse woman choos work hooter okay</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>call sexist want girl never comment footbal me...</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>live long prosper leonard nimoy repo dead 83</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>sexist right hate girl</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        clean_tweet   Hash Words\n",
       "0      0                  ok enough instant restaur alreadi  No hashtags\n",
       "1      0     femal cowork asse woman choos work hooter okay  No hashtags\n",
       "2      1  call sexist want girl never comment footbal me...  No hashtags\n",
       "3      0       live long prosper leonard nimoy repo dead 83  No hashtags\n",
       "4      1                             sexist right hate girl  No hashtags"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copen = pd.read_csv('data/Cleaning/Copenhagen_Dataset.csv',on_bad_lines='skip')\n",
    "copen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b90a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6698\n",
       "1    2570\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copen.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10007b6a",
   "metadata": {},
   "source": [
    "# Train and Test Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d358eb9",
   "metadata": {},
   "source": [
    "### Hugging Face Dataset to Train and Others to Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900d3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "copen_x_train = copen.clean_tweet\n",
    "hasoc_x_test = hasoc.clean_tweet\n",
    "aris_x_test = aris.clean_tweet\n",
    "hf_x_test = hf.clean_tweet\n",
    "\n",
    "copen_y_train = copen.label\n",
    "hf_y_test = hf.label\n",
    "hasoc_y_test = hasoc.label\n",
    "aris_y_test = aris.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e289a3",
   "metadata": {},
   "source": [
    "### Implement TFIDF for Unshuffled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4316e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "tfidf_tr = vec.fit_transform(copen_x_train)\n",
    "tfidf_hasoc_test = vec.transform(hasoc_x_test)\n",
    "tfidf_aris_test = vec.transform(aris_x_test)\n",
    "tfidf_hf_test = vec.transform(hf_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138614a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9268, 9293)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d884c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_document_vector=tfidf_hasoc_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32fa51a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7316)\t0.28428519424008153\n",
      "  (0, 6322)\t0.4789268289653885\n",
      "  (0, 4352)\t0.3334796045453805\n",
      "  (0, 3555)\t0.3206900897795523\n",
      "  (0, 3389)\t0.4567291734612506\n",
      "  (0, 868)\t0.516873971691242\n"
     ]
    }
   ],
   "source": [
    "print(first_document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c5e817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bastard</th>\n",
       "      <td>0.516874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prime</th>\n",
       "      <td>0.478927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genuin</th>\n",
       "      <td>0.456729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>0.333480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.320690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>0.284285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pommi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pommiesfohewin</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ponder</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poni</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poodl</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poofling</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pomegran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poorer</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poorli</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poorsposmanship</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poop</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polydetestr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polygon</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poke</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pokemon</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pokemonprob</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poker</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poland</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polar</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polic</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polygam</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polici</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polka</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poll</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polli</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pope</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popcorn</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popehat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poion</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poppet</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postthat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pot</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tfidf\n",
       "bastard          0.516874\n",
       "prime            0.478927\n",
       "genuin           0.456729\n",
       "job              0.333480\n",
       "great            0.320690\n",
       "show             0.284285\n",
       "pommi            0.000000\n",
       "pommiesfohewin   0.000000\n",
       "ponder           0.000000\n",
       "poni             0.000000\n",
       "poo              0.000000\n",
       "poodl            0.000000\n",
       "000              0.000000\n",
       "poofling         0.000000\n",
       "pomegran         0.000000\n",
       "poor             0.000000\n",
       "poorer           0.000000\n",
       "poorli           0.000000\n",
       "poorsposmanship  0.000000\n",
       "pop              0.000000\n",
       "poop             0.000000\n",
       "polydetestr      0.000000\n",
       "polygon          0.000000\n",
       "pole             0.000000\n",
       "poke             0.000000\n",
       "pokemon          0.000000\n",
       "pokemonprob      0.000000\n",
       "poker            0.000000\n",
       "poland           0.000000\n",
       "polar            0.000000\n",
       "polic            0.000000\n",
       "polygam          0.000000\n",
       "polici           0.000000\n",
       "polit            0.000000\n",
       "polka            0.000000\n",
       "poll             0.000000\n",
       "polli            0.000000\n",
       "pope             0.000000\n",
       "popcorn          0.000000\n",
       "popehat          0.000000\n",
       "poion            0.000000\n",
       "poppet           0.000000\n",
       "postthat         0.000000\n",
       "pot              0.000000\n",
       "potat            0.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vec.get_feature_names()\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b30dbc",
   "metadata": {},
   "source": [
    "### Shuffle the Data for Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d03e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_shuffle = hf.sample(frac=1)\n",
    "hasoc_shuffle = hasoc.sample(frac=1)\n",
    "aris_shuffle = aris.sample(frac=1)\n",
    "copen_shuffle = copen.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6979fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "copen_x_train_shuffle = copen_shuffle.clean_tweet\n",
    "hasoc_x_test_shuffle = hasoc_shuffle.clean_tweet\n",
    "aris_x_test_shuffle = aris_shuffle.clean_tweet\n",
    "hf_x_test_shuffle = hf_shuffle.clean_tweet\n",
    "\n",
    "copen_y_train_shuffle = copen_shuffle.label\n",
    "hasoc_y_test_shuffle = hasoc_shuffle.label\n",
    "aris_y_test_shuffle = aris_shuffle.label\n",
    "hf_y_test_shuffle = hf_shuffle.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff306674",
   "metadata": {},
   "source": [
    "### Implement TFIDF for Shuffled Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e35eacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "tfidf_tr_shuffle = vec.fit_transform(copen_x_train_shuffle)\n",
    "tfidf_hasoc_test_shuffle = vec.transform(hasoc_x_test_shuffle)\n",
    "tfidf_aris_test_shuffle = vec.transform(aris_x_test_shuffle)\n",
    "tfidf_hf_test_shuffle = vec.transform(hf_x_test_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5d90c",
   "metadata": {},
   "source": [
    "# Artifical Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "copen_y_train_encode = preprocessing.LabelEncoder().fit_transform(copen_y_train_shuffle)\n",
    "hasoc_y_test_encode = preprocessing. LabelEncoder().fit_transform(hasoc_y_test_shuffle)\n",
    "aris_y_test_encode = preprocessing. LabelEncoder().fit_transform(aris_y_test_shuffle)\n",
    "hf_y_test_encode = preprocessing. LabelEncoder().fit_transform(hf_y_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b575c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "copen_x_train = scipy.sparse.csr_matrix.todense(tfidf_tr_shuffle)\n",
    "hasoc_x_test = scipy.sparse.csr_matrix.todense(tfidf_hasoc_test_shuffle)\n",
    "aris_x_test = scipy.sparse.csr_matrix.todense(tfidf_aris_test_shuffle)\n",
    "hf_x_test = scipy.sparse.csr_matrix.todense(tfidf_hf_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ffe4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the model\n",
    "def get_model(trainX,trainy):\n",
    " # define model\n",
    "    model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(128, activation='relu', input_shape=(trainX.shape[1],)),\n",
    "                            tf.keras.layers.Dropout(0.2),\n",
    "                            tf.keras.layers.Dense(32, activation='relu'),\n",
    "                            tf.keras.layers.Dropout(0.2),\n",
    "                            tf.keras.layers.Dense(128, activation='softmax')])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(trainX, trainy, epochs=10, verbose=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bacc9b",
   "metadata": {},
   "source": [
    "### Use HASOC to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03b1b7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 22:12:41.710196: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "290/290 - 1s - loss: 1.3059 - accuracy: 0.7299 - 1s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "290/290 - 1s - loss: 0.3437 - accuracy: 0.8608 - 812ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "290/290 - 1s - loss: 0.2349 - accuracy: 0.9097 - 786ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "290/290 - 1s - loss: 0.1710 - accuracy: 0.9376 - 797ms/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "290/290 - 1s - loss: 0.1341 - accuracy: 0.9530 - 788ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "290/290 - 1s - loss: 0.1062 - accuracy: 0.9619 - 784ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "290/290 - 1s - loss: 0.0872 - accuracy: 0.9698 - 790ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "290/290 - 1s - loss: 0.0736 - accuracy: 0.9719 - 783ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "290/290 - 1s - loss: 0.0597 - accuracy: 0.9783 - 796ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "290/290 - 1s - loss: 0.0543 - accuracy: 0.9815 - 782ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "ann_model = get_model(copen_x_train,copen_y_train_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "030ff176",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hasoc_predict = ann_model.predict(hasoc_x_test,verbose=0)\n",
    "y_predict_hasoc_classes=np.argmax(y_hasoc_predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1f7f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6793657784628936\n",
      "Recall:  0.18092691622103388\n",
      "Precision:  0.2743243243243243\n",
      "F1 Score:  0.2180451127819549\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(hasoc_y_test_encode, y_predict_hasoc_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b72bd",
   "metadata": {},
   "source": [
    "### Use Aristotle Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63136657",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aris_predict = ann_model.predict(aris_x_test,verbose=0)\n",
    "y_predict_aris_classes=np.argmax(y_aris_predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "386d3623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7307692307692307\n",
      "Recall:  0.21261378413524057\n",
      "Precision:  0.3367662203913491\n",
      "F1 Score:  0.2606616181745715\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(aris_y_test_encode, y_predict_aris_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7e954",
   "metadata": {},
   "source": [
    "### Use Hugging Face Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed0b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hf_predict = ann_model.predict(hf_x_test,verbose=0)\n",
    "y_predict_hf_classes=np.argmax(y_hf_predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a618f8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8543685715346626\n",
      "Recall:  0.24105011933174225\n",
      "Precision:  0.13228552717747216\n",
      "F1 Score:  0.1708245243128964\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(hf_y_test_encode, y_predict_hf_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4d0d1",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f2d8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100).fit(tfidf_tr, copen_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ca6b7",
   "metadata": {},
   "source": [
    "### Use HASOC to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d39887b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7381634001321294\n",
      "Recall:  0.0374331550802139\n",
      "Precision:  0.2781456953642384\n",
      "F1 Score:  0.06598586017282011\n"
     ]
    }
   ],
   "source": [
    "hasoc_rf_test = rf.predict(tfidf_hasoc_test)\n",
    "get_metrics_confusion(hasoc_y_test, hasoc_rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49c4fa",
   "metadata": {},
   "source": [
    "### Use Aristotle Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b56ab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7702467343976778\n",
      "Recall:  0.06501950585175553\n",
      "Precision:  0.40816326530612246\n",
      "F1 Score:  0.11217049915872127\n"
     ]
    }
   ],
   "source": [
    "aris_rf_test = rf.predict(tfidf_aris_test)\n",
    "get_metrics_confusion(aris_y_test, aris_rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cea9a7",
   "metadata": {},
   "source": [
    "### Use Hugging Face Dataset to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f41b73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9141138464966024\n",
      "Recall:  0.07040572792362769\n",
      "Precision:  0.13516609392898052\n",
      "F1 Score:  0.09258532757944292\n"
     ]
    }
   ],
   "source": [
    "hf_rf_test = rf.predict(tfidf_hf_test)\n",
    "get_metrics_confusion(hf_y_test, hf_rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e2aa1",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcd2bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.LinearSVC(random_state=42).fit(tfidf_tr, copen_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94cc5f",
   "metadata": {},
   "source": [
    "### Use HASOC to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccfc75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7093151288262497\n",
      "Recall:  0.12477718360071301\n",
      "Precision:  0.2928870292887029\n",
      "F1 Score:  0.175\n"
     ]
    }
   ],
   "source": [
    "hasoc_svc_test = svc.predict(tfidf_hasoc_test)\n",
    "get_metrics_confusion(hasoc_y_test, hasoc_svc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f16b2e",
   "metadata": {},
   "source": [
    "### Use Aristotle Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a971d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7516690856313498\n",
      "Recall:  0.12288686605981794\n",
      "Precision:  0.3430127041742287\n",
      "F1 Score:  0.18094782192436573\n"
     ]
    }
   ],
   "source": [
    "aris_svc_test = svc.predict(tfidf_aris_test)\n",
    "get_metrics_confusion(aris_y_test, aris_svc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3fecd",
   "metadata": {},
   "source": [
    "### Use Hugging Face Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bea9dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9030113995024321\n",
      "Recall:  0.1575178997613365\n",
      "Precision:  0.18032786885245902\n",
      "F1 Score:  0.1681528662420382\n"
     ]
    }
   ],
   "source": [
    "hf_svc_test = svc.predict(tfidf_hf_test)\n",
    "get_metrics_confusion(hf_y_test, hf_svc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e39d92",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7f67d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression().fit(tfidf_tr, copen_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dd962",
   "metadata": {},
   "source": [
    "### Use HASOC to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2723f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasoc_log_test = log.predict(tfidf_hasoc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41c7d89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7485135432724069\n",
      "Recall:  0.0106951871657754\n",
      "Precision:  0.2727272727272727\n",
      "F1 Score:  0.02058319039451115\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(hasoc_y_test, hasoc_log_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7b617",
   "metadata": {},
   "source": [
    "### Use Aristotle Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53014a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aris_log_test = log.predict(tfidf_aris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74637ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7799709724238026\n",
      "Recall:  0.03446033810143043\n",
      "Precision:  0.6309523809523809\n",
      "F1 Score:  0.06535141800246609\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(aris_y_test, aris_log_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2978d07",
   "metadata": {},
   "source": [
    "### Use Hugging Face Dataset to Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed9a7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_log_test = log.predict(tfidf_hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b40403fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9331253945267536\n",
      "Recall:  0.04116945107398568\n",
      "Precision:  0.2623574144486692\n",
      "F1 Score:  0.07117070654976793\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(hf_y_test, hf_log_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dec59f",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a83ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier().fit(tfidf_tr, copen_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e7a60",
   "metadata": {},
   "source": [
    "### Use HASOC to Test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80508878",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasoc_gbc_test = gbc.predict(tfidf_hasoc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0599fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7509359171988549\n",
      "Recall:  0.006238859180035651\n",
      "Precision:  0.30434782608695654\n",
      "F1 Score:  0.01222707423580786\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(hasoc_y_test, hasoc_gbc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58252753",
   "metadata": {},
   "source": [
    "### Use Aristotle Dataset to Test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fde8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "aris_gbc_test = gbc.predict(tfidf_aris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "197e571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7769230769230769\n",
      "Recall:  0.009102730819245773\n",
      "Precision:  0.5185185185185185\n",
      "F1 Score:  0.017891373801916934\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(aris_y_test, aris_gbc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50ee6d",
   "metadata": {},
   "source": [
    "### Use Hugging Face Dataset to Test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "513c9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_gbc_test = gbc.predict(tfidf_hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0404dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9362816085551966\n",
      "Recall:  0.016706443914081145\n",
      "Precision:  0.2916666666666667\n",
      "F1 Score:  0.03160270880361174\n"
     ]
    }
   ],
   "source": [
    "get_metrics_confusion(hf_y_test, hf_gbc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
